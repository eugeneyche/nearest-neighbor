
%-----------------------------
%	PACKAGES
%-----------------------------

\documentclass[paper=letter, fontsize=12pt]{article} % Letter paper and 11pt font size

\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{setspace}
\usepackage{sectsty} % Allows customizing section commands

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{0pt} % Customize the height of the header

%margin
\addtolength{\textwidth}{1.5in}
\addtolength{\textheight}{1.50in}
\addtolength{\hoffset}{-0.75in}
\addtolength{\voffset}{-0.75in}


%-----------------------------
%	TITLE SECTION
%-----------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{
\normalfont \normalsize 
\horrule{2pt} \\
\LARGE Nearest Neighbor Search Data Structure \\ [-10 pt] %title
\horrule{1pt} \\
}

\author{
	\small
	\begin{tabular}{*{3}{c}}
		\textbf{Sanjoy Dasgupta} & \textbf{Zhen Zhai} & \textbf{Eugene Che} \\
		Department of Computer Science & Department of Computer Science & Department of Computer Science  \\
		University of California, San Diego & University of California, San Diego & University of California, San Diego \\
		dasgupta@cs.ucsd.edu & zzhai@ucsd.edu & eche@ucsd.edu \\
	\end{tabular}
}
 
\date{}

\begin{document}

\maketitle

\begin{abstract}
Nearest-neighbor(NN) search is boardly used within all different fields of study to gain information on new data from training data set. For NN search, the more complex the training data set is the more accurate the result will be. However, doing NN search on complex and large training data sets is time consuming. Therefore, improving the speed and acuracy of nearest-neighbor search becomes essential. We look at different data structures for NN search and compare the results on varied data sets. We focus on data structures including kd tree, kd spill tree, kd virtual spill tree, bsp tree, and bsp spill tree. We conclude that spilling often improve the performance of the data structures.
\end{abstract}

\section{Introduction}
Gaining information of an input query by looking for the most similar match from the training set is referred as nearest neighbor(NN) search. We are given a data set $\mathbf S$ that contains a set of points $\{ s_1, s_2, s_3, ... , s_n\}$ and an input query $q$, and we are looking for the nearest neighbor $s_i$ of $q$ in $\mathbf S$ such that $s_i$ can provide valuable information about input query $q$.
\\~\\
A lot of machine learning algorithms spend a big amount of time on nearest neighbor search. The reason that NN search is time consuming is that the training set could very well be represented by high dimentional vectors. In fact, the accuracy of the NN search result is proportional with the complexity of the training set. Therefore, speeding up NN search can lead to tremoudous improvment to many applications of not only machine learning but also data analysis, bio-informatics, signal processing, etc.
\\~\\
There has been a large number of research devoting to speed up the performance of NN search. In this paper, we evaluate some most popular data structures and compare their result. We apply these data structures on five different data sets from different area of study including computer vision, human relationship...

\section{Data structures}
We focus our study on binary space partitioning(BSP) trees, one of the most promising data structure for NN search. We first look at one specific BSP tree: k-dimentional(KD) trees. Then we study a more general BSP tree, in which we preprocess the data before partitioning the space. Finally, we look at spill trees and virtual spill trees, which are the revised data structures of BSP trees. Experiment supports that spilling improve the accuracy of NN search and virtual spilling improve the performance even more.

\subsection{KD trees}
KD tree is 
\subsection{BSP tree??}
\subsection{Spill trees and virtual spill trees}

\section{Experimental results}
\subsection{Mnist}
\subsection{CIFAR}
\subsection{eHarmony}
\subsection{data4}
\subsection{data5}

\section{Conclusion}

\section{References}







\end{document}